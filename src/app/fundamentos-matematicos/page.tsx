'use client';
import CodeBlock from "@/components/CodeBlock";
import CollapsibleSection from "@/components/CollapsibleSection";
import DerivativeGraph from "@/components/DerivativeGraph";
import GradientDescentSimulation from "@/components/GradientDescentSimulation";
import InteractiveGraph from "@/components/InteractiveGraph";
import VectorVisualization from "@/components/VectorVisualization";
import Link from "next/link";

export default function FundamentosMatematicos() {
  return (
    <div className="container">
      <h1>üßÆ Fundamentos Matem√°ticos</h1>
      <p style={{ fontSize: '1.2rem', maxWidth: '800px' }}>
        Las matem√°ticas son la base de todo en deep learning. Sin entender derivadas, 
        no puedes entender backpropagation. Sin entender vectores, no puedes entender 
        embeddings. Esta secci√≥n te dar√° las bases s√≥lidas que necesitas.
      </p>

      <div className="deep-explanation">
        <h4>üéì Filosof√≠a de esta Secci√≥n</h4>
        <p>
          Como f√≠sico, creo firmemente que <strong>visualizar las funciones es esencial</strong> para 
          entender su comportamiento. Por eso, cada concepto incluye gr√°ficas interactivas donde 
          puedes modificar par√°metros y ver qu√© pasa. Juega con ellas.
        </p>
        <p style={{ marginBottom: 0 }}>
          Las secciones colapsables (üìñ) contienen explicaciones m√°s profundas y derivaciones 
          matem√°ticas. Son opcionales pero recomendadas si quieres entender el &quot;por qu√©&quot;.
        </p>
      </div>

      {/* Tabla de Contenidos */}
      <div className="toc">
        <div className="toc-title">üìë Contenido</div>
        <ul className="toc-list">
          <li className="toc-item"><a href="#funciones">1. Funciones Matem√°ticas</a></li>
          <li className="toc-item"><a href="#derivadas">2. Derivadas (¬°Fundamental!)</a></li>
          <li className="toc-item"><a href="#vectores">3. Vectores y Producto Escalar</a></li>
          <li className="toc-item"><a href="#gradientes">4. Gradientes y Descenso del Gradiente</a></li>
          <li className="toc-item"><a href="#matrices">5. Matrices y Redes Neuronales</a></li>
          <li className="toc-item"><a href="#probabilidad">6. Probabilidad y Estad√≠stica</a></li>
        </ul>
      </div>

      {/* ========================================
          SECCI√ìN 1: FUNCIONES MATEM√ÅTICAS
          ======================================== */}
      <section id="funciones" className="section">
        <h2>1. Funciones Matem√°ticas</h2>
        
        <p>
          Una funci√≥n es una &quot;m√°quina&quot; que toma un n√∫mero como entrada (x) y produce 
          otro n√∫mero como salida (y). En deep learning, las funciones est√°n en todas partes: 
          funciones de activaci√≥n, funciones de p√©rdida, transformaciones...
        </p>

        <h3>1.1 Funci√≥n Lineal: y = kx</h3>
        <p>
          La funci√≥n m√°s simple. Multiplicar la entrada por una constante k (la pendiente). 
          Esta es la base de las transformaciones lineales en las capas de una red neuronal.
        </p>
        
        <InteractiveGraph type="linear" title="Gr√°fica Interactiva: Funci√≥n Lineal" />

        <CodeBlock
          title="funciones_lineales.py"
          code={`import numpy as np
import matplotlib.pyplot as plt

# Ejemplo: y = 2 * x
# Si x = 1, entonces y = 2
# Si x = 5, entonces y = 10

x = 1
y = 2 * x
print(f"Cuando x = {x}, y = {y}")

x = 5
y = 2 * x
print(f"Cuando x = {x}, y = {y}")

# Visualizaci√≥n con diferentes pendientes
x_values = np.linspace(-5, 5, 100)
k_values = [0.5, 1, 2, 3, -1]

plt.figure(figsize=(10, 6))
for k in k_values:
    y_values = k * x_values
    plt.plot(x_values, y_values, label=f'y = {k}x', linewidth=2)

plt.xlabel('x')
plt.ylabel('y')
plt.title('Funciones Lineales: y = kx')
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()`}
        />

        <CollapsibleSection title="Entender la pendiente k" icon="üìñ">
          <p>
            La <strong>pendiente k</strong> determina qu√© tan r√°pido sube (o baja) la recta:
          </p>
          <ul>
            <li><strong>k &gt; 0</strong>: La recta sube de izquierda a derecha.</li>
            <li><strong>k &lt; 0</strong>: La recta baja de izquierda a derecha.</li>
            <li><strong>k = 0</strong>: La recta es horizontal (salida constante 0).</li>
            <li><strong>|k| grande</strong>: M√°s inclinada.</li>
          </ul>
          <p>
            En una neurona, los pesos (weights) funcionan como estas pendientes: escalan 
            la entrada antes de sumarla con otras.
          </p>
        </CollapsibleSection>

        <h3>1.2 Funci√≥n Cuadr√°tica: y = x¬≤</h3>
        <p>
          Elevar al cuadrado la entrada. Si x = 2, entonces y = 4. Si x = -2, tambi√©n y = 4.
          Los negativos se vuelven positivos. Esto es crucial para funciones de p√©rdida como 
          MSE (Mean Squared Error), donde queremos penalizar errores sin importar su signo.
        </p>
        
        <InteractiveGraph type="quadratic" title="Gr√°fica: y = x¬≤" />


        <CodeBlock
          title="funciones_cuadraticas.py"
          code={`import numpy as np
import matplotlib.pyplot as plt

# y = x¬≤
examples = [-3, -2, -1, 0, 1, 2, 3]
for x in examples:
    y = x ** 2
    print(f"Cuando x = {x}, y = x¬≤ = {y}")

# Nota: los negativos tambi√©n dan positivo!
# Esto es √∫til para funciones de p√©rdida (MSE)

# Visualizaci√≥n
x_values = np.linspace(-5, 5, 100)
y_values = x_values ** 2

plt.figure(figsize=(8, 5))
plt.plot(x_values, y_values, 'r-', linewidth=2, label='y = x¬≤')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Funci√≥n Cuadr√°tica: y = x¬≤')
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()`}
        />

        <h3>1.3 Funci√≥n Exponencial: y = e^x</h3>
        <p>
          La exponencial crece explosivamente. Es la base de softmax y de muchas distribuciones 
          de probabilidad.
        </p>
        
        <InteractiveGraph type="exponential" title="Gr√°fica Interactiva: Exponencial" />

        <h3>1.4 Funci√≥n Sigmoide</h3>
        <div className="concept-box">
          <div className="concept-title">üî• Muy Importante</div>
          <p style={{ margin: 0 }}>
            La sigmoide &quot;aplasta&quot; cualquier n√∫mero real al rango (0, 1). 
            Perfecta para representar probabilidades y para clasificaci√≥n binaria.
          </p>
        </div>
        
        <InteractiveGraph type="sigmoid" title="Gr√°fica Interactiva: Sigmoide" />

        <CodeBlock
          title="funcion_sigmoide.py"
          code={`import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    \"\"\"
    Funci√≥n Sigmoide: œÉ(x) = 1 / (1 + e^(-x))
    
    - Cuando x ‚Üí ‚àû, œÉ(x) ‚Üí 1
    - Cuando x ‚Üí -‚àû, œÉ(x) ‚Üí 0
    - Cuando x = 0, œÉ(x) = 0.5
    \"\"\"
    return 1 / (1 + np.exp(-x))

# Ejemplos
print(f\"sigmoid(-10) = {sigmoid(-10):.6f}\")  # Cercano a 0
print(f\"sigmoid(0) = {sigmoid(0):.6f}\")      # Exactamente 0.5
print(f\"sigmoid(10) = {sigmoid(10):.6f}\")    # Cercano a 1

# Visualizaci√≥n
x = np.linspace(-10, 10, 100)
y = sigmoid(x)

plt.figure(figsize=(10, 6))
plt.plot(x, y, 'b-', linewidth=2, label='œÉ(x)')
plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='y = 0.5')
plt.xlabel('x')
plt.ylabel('œÉ(x)')
plt.title('Funci√≥n Sigmoide')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()`}
        />

        <CollapsibleSection title="La f√≥rmula de la sigmoide y por qu√© funciona" icon="üìñ">
          <div className="math-block">
            œÉ(x) = 1 / (1 + e^(-x))
          </div>
          <p>
            Observa que cuando x es muy grande y positivo, e^(-x) ‚Üí 0, as√≠ que œÉ(x) ‚Üí 1.
            Cuando x es muy grande y negativo, e^(-x) ‚Üí ‚àû, as√≠ que œÉ(x) ‚Üí 0.
            Y cuando x = 0, e^0 = 1, as√≠ que œÉ(0) = 0.5.
          </p>
          <p>
            Esta propiedad de &quot;saturaci√≥n&quot; en los extremos puede causar problemas 
            de gradientes que desaparecen (vanishing gradients), por lo que hoy se prefiere 
            ReLU en capas ocultas.
          </p>
        </CollapsibleSection>

        <h3>1.5 Funci√≥n ReLU</h3>
        <p>
          ReLU (Rectified Linear Unit) es la funci√≥n de activaci√≥n m√°s usada hoy. 
          Simplemente: si x &lt; 0, devuelve 0. Si x ‚â• 0, devuelve x.
        </p>
        
        <InteractiveGraph type="relu" title="Gr√°fica Interactiva: ReLU" />
      </section>

      {/* ========================================
          SECCI√ìN 2: DERIVADAS
          ======================================== */}
      <section id="derivadas" className="section">
        <h2>2. Derivadas</h2>

        <div className="deep-explanation">
          <h4>‚ö†Ô∏è ¬°Esto es FUNDAMENTAL!</h4>
          <p>
            Si no entiendes derivadas conceptualmente, <strong>nunca</strong> entender√°s 
            c√≥mo aprenden las redes neuronales. Backpropagation (el algoritmo que entrena 
            las redes) es esencialmente calcular derivadas en cadena.
          </p>
        </div>

        <h3>2.1 ¬øQu√© es una derivada? (La intuici√≥n)</h3>
        <p>
          Imagina que est√°s subiendo una colina. La <strong>derivada</strong> te dice 
          qu√© tan empinada est√° la pendiente en el punto exacto donde est√°s parado.
        </p>
        
        <div className="concept-box">
          <div className="concept-title">üìê Definici√≥n Intuitiva</div>
          <p style={{ margin: 0 }}>
            La derivada mide <strong>cu√°nto cambia la salida (y) cuando la entrada (x) 
            cambia un poquito</strong>. Es la &quot;tasa de cambio instant√°nea&quot;.
          </p>
        </div>

        <p>
          Matem√°ticamente, si incrementamos x por una cantidad muy peque√±a h, miramos 
          cu√°nto cambi√≥ y, y dividimos:
        </p>

        <div className="math-block">
          f&apos;(x) = lim (h‚Üí0) [ f(x + h) - f(x) ] / h
        </div>

        <p>
          Cuando h es muy peque√±o, esta fracci√≥n nos da la pendiente <strong>exacta</strong> 
          de la funci√≥n en ese punto.
        </p>

        <DerivativeGraph />

        <CodeBlock
          title="derivada_numerica.py"
          code={`import numpy as np

# La derivada es el l√≠mite cuando h ‚Üí 0 de:
# f'(x) = (f(x + h) - f(x)) / h

def derivada_numerica(f, x, h=1e-7):
    """Calcula la derivada de f en x usando diferencias finitas."""
    return (f(x + h) - f(x)) / h

# Ejemplo: f(x) = x¬≤
# La derivada anal√≠tica es f'(x) = 2x
def f(x):
    return x ** 2

# Verificar en x = 3
# f'(3) deber√≠a ser 2 * 3 = 6
x = 3
derivada = derivada_numerica(f, x)
print(f"Derivada num√©rica de x¬≤ en x=3: {derivada:.6f}")
print(f"Derivada anal√≠tica (2x): {2 * x}")`}
        />

        <CollapsibleSection title="Entendiendo la definici√≥n paso a paso" icon="üìñ">
          <p>
            Vamos a desglosar la definici√≥n de derivada:
          </p>
          <ol>
            <li>
              <strong>f(x + h) - f(x)</strong>: Es cu√°nto &quot;subi√≥&quot; o &quot;baj√≥&quot; la funci√≥n 
              cuando movimos la entrada de x a x + h.
            </li>
            <li>
              <strong>/ h</strong>: Dividimos por cu√°nto nos movimos. Esto nos da la 
              &quot;raz√≥n de cambio&quot; - cu√°nto cambi√≥ y POR CADA unidad que cambi√≥ x.
            </li>
            <li>
              <strong>lim (h‚Üí0)</strong>: Tomamos el l√≠mite cuando h se hace infinitamente 
              peque√±o. Esto convierte la &quot;pendiente promedio&quot; entre dos puntos en la 
              &quot;pendiente instant√°nea&quot; en un solo punto.
            </li>
          </ol>
          <p>
            <strong>Analog√≠a:</strong> Si conduces 100 km en 2 horas, tu velocidad promedio 
            es 50 km/h. Pero la derivada ser√≠a tu velocidad instant√°nea en un momento exacto 
            (lo que marca el veloc√≠metro).
          </p>
        </CollapsibleSection>

        <h3>2.2 Reglas de Derivaci√≥n</h3>
        <p>
          Afortunadamente, no tenemos que calcular el l√≠mite cada vez. Hay reglas:
        </p>

        <CodeBlock
          title="reglas_derivadas.py"
          code={`# Reglas fundamentales de derivaci√≥n:

# 1. Constante: d/dx[c] = 0
#    La derivada de un n√∫mero fijo es 0 (no cambia)

# 2. Potencia: d/dx[x^n] = n * x^(n-1)
#    Ejemplo: d/dx[x¬≤] = 2x
#    Ejemplo: d/dx[x¬≥] = 3x¬≤

# 3. Suma: d/dx[f + g] = df/dx + dg/dx
#    La derivada de una suma es la suma de las derivadas

# 4. Producto: d/dx[f * g] = f'g + fg'
#    Un poco m√°s complicado, pero importante

# 5. Cadena: d/dx[f(g(x))] = f'(g(x)) * g'(x)
#    ¬°LA M√ÅS IMPORTANTE PARA DEEP LEARNING!`}
        />

        <h3>2.3 La Regla de la Cadena</h3>
        <div className="concept-box">
          <div className="concept-title">‚ö° El Coraz√≥n de Backpropagation</div>
          <p style={{ margin: 0 }}>
            La regla de la cadena nos dice c√≥mo derivar funciones compuestas (funciones 
            dentro de funciones). En una red neuronal, los datos pasan por muchas capas 
            - eso es una funci√≥n compuesta gigante.
          </p>
        </div>

        <div className="math-block">
          Si y = f(g(x)), entonces dy/dx = f&apos;(g(x)) ¬∑ g&apos;(x)
        </div>

        <CollapsibleSection title="Ejemplo: C√≥mo funciona la regla de la cadena" icon="üìñ">
          <p>
            Supongamos que tenemos h(x) = (x¬≤ + 1)¬≥
          </p>
          <p>
            Podemos ver esto como dos funciones anidadas:
          </p>
          <ul>
            <li>Funci√≥n &quot;interior&quot;: g(x) = x¬≤ + 1</li>
            <li>Funci√≥n &quot;exterior&quot;: f(u) = u¬≥ (donde u = g(x))</li>
          </ul>
          <p>
            Ahora aplicamos la regla de la cadena:
          </p>
          <ol>
            <li>Derivada de f: f&apos;(u) = 3u¬≤ = 3(x¬≤ + 1)¬≤</li>
            <li>Derivada de g: g&apos;(x) = 2x</li>
            <li>Multiplicamos: h&apos;(x) = 3(x¬≤ + 1)¬≤ ¬∑ 2x = 6x(x¬≤ + 1)¬≤</li>
          </ol>
          <p>
            <strong>En backpropagation</strong>, hacemos exactamente esto pero hacia atr√°s: 
            empezamos con el error final y &quot;propagamos&quot; las derivadas hacia atr√°s a 
            trav√©s de cada capa usando la regla de la cadena.
          </p>
        </CollapsibleSection>
      </section>

      {/* ========================================
          SECCI√ìN 3: VECTORES
          ======================================== */}
      <section id="vectores" className="section">
        <h2>3. Vectores y Producto Escalar</h2>

        <p>
          Un vector es simplemente una lista ordenada de n√∫meros. En deep learning, 
          los datos casi siempre est√°n en forma de vectores (o matrices/tensores).
        </p>

        <div className="concept-box">
          <div className="concept-title">üìö Nota Importante</div>
          <p style={{ margin: 0 }}>
            Esta secci√≥n cubre lo m√≠nimo necesario. Para una comprensi√≥n profunda, 
            recomiendo completar un curso de √Ålgebra Lineal (3Blue1Brown tiene uno 
            excelente y visual en YouTube: &quot;Essence of Linear Algebra&quot;).
          </p>
        </div>

        <h3>3.1 El Producto Escalar (Dot Product)</h3>
        <p>
          El producto escalar es <strong>LA operaci√≥n</strong> m√°s importante en redes 
          neuronales. Cada neurona b√°sicamente calcula un producto escalar entre sus 
          entradas y sus pesos.
        </p>

        <div className="math-block">
          a ¬∑ b = a‚ÇÅb‚ÇÅ + a‚ÇÇb‚ÇÇ + ... + a‚Çôb‚Çô = |a| |b| cos(Œ∏)
        </div>

        <CodeBlock
          title="operaciones_vectores.py"
          code={`import numpy as np

# Crear vectores
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])

print("Vector v1:", v1)
print("Vector v2:", v2)

# Operaciones b√°sicas
print("\\n--- Operaciones B√°sicas ---")
print("Suma v1 + v2:", v1 + v2)
print("Resta v1 - v2:", v1 - v2)
print("Multiplicaci√≥n elemento a elemento:", v1 * v2)

# Producto escalar (dot product) - ¬°MUY IMPORTANTE!
dot_product = np.dot(v1, v2)  # = 1*4 + 2*5 + 3*6 = 32
print("\\n--- Producto Escalar ---")
print("v1 ¬∑ v2 =", dot_product)

# Norma (magnitud del vector)
norma = np.linalg.norm(v1)
print("\\n--- Norma ---")
print("||v1|| =", norma)`}
        />

        <VectorVisualization />

        <CollapsibleSection title="¬øPor qu√© el producto escalar mide similitud?" icon="üìñ">
          <p>
            La f√≥rmula <code>a ¬∑ b = |a| |b| cos(Œ∏)</code> nos revela algo profundo:
          </p>
          <ul>
            <li>
              <strong>cos(0¬∞) = 1</strong>: Vectores paralelos (misma direcci√≥n) ‚Üí producto m√°ximo.
            </li>
            <li>
              <strong>cos(90¬∞) = 0</strong>: Vectores perpendiculares ‚Üí producto cero.
            </li>
            <li>
              <strong>cos(180¬∞) = -1</strong>: Vectores opuestos ‚Üí producto negativo.
            </li>
          </ul>
          <p>
            Por eso el <strong>Cosine Similarity</strong> (producto escalar normalizado) 
            se usa tanto en NLP y sistemas de b√∫squeda. Si tienes dos embeddings de 
            oraciones y su cosine similarity es alta, las oraciones son &quot;similares&quot; 
            sem√°nticamente.
          </p>
          <p>
            En RAG (Retrieval-Augmented Generation), buscamos documentos cuyo embedding 
            tenga alto cosine similarity con la pregunta del usuario.
          </p>
        </CollapsibleSection>

        <h3>3.2 ¬øQu√© es una Neurona Artificial?</h3>
        
        <p>
          Una <strong>neurona artificial</strong> es la unidad b√°sica de una red neuronal. 
          Est√° inspirada (vagamente) en las neuronas biol√≥gicas: recibe se√±ales de entrada, 
          las procesa, y produce una salida.
        </p>

        <div className="concept-box">
          <div className="concept-title">üß† Anatom√≠a de una Neurona</div>
          <p style={{ margin: 0 }}>
            Una neurona recibe m√∫ltiples <strong>entradas</strong> (x‚ÇÅ, x‚ÇÇ, ..., x‚Çô), 
            cada una multiplicada por un <strong>peso</strong> (w‚ÇÅ, w‚ÇÇ, ..., w‚Çô). 
            Suma todo, a√±ade un <strong>sesgo</strong> (b), y pasa el resultado por 
            una <strong>funci√≥n de activaci√≥n</strong>.
          </p>
        </div>

        {/* Diagrama de Neurona estilo ASCII/SVG */}
        <div className="interactive-graph">
          <h4 style={{ marginTop: 0, marginBottom: '1rem', color: 'var(--highlight)', textAlign: 'center' }}>
            Diagrama de una Neurona
          </h4>
          <div style={{ 
            display: 'flex', 
            justifyContent: 'center', 
            alignItems: 'center',
            padding: '1.5rem',
            background: 'rgba(0,0,0,0.3)',
            borderRadius: '8px'
          }}>
            <svg viewBox="0 0 400 180" style={{ maxWidth: '500px', width: '100%' }}>
              {/* Entradas */}
              <circle cx="50" cy="40" r="20" fill="none" stroke="#7c3aed" strokeWidth="2"/>
              <text x="50" y="45" textAnchor="middle" fill="#f0f0f0" fontSize="14">x‚ÇÅ</text>
              
              <circle cx="50" cy="90" r="20" fill="none" stroke="#7c3aed" strokeWidth="2"/>
              <text x="50" y="95" textAnchor="middle" fill="#f0f0f0" fontSize="14">x‚ÇÇ</text>
              
              <circle cx="50" cy="140" r="20" fill="none" stroke="#7c3aed" strokeWidth="2"/>
              <text x="50" y="145" textAnchor="middle" fill="#f0f0f0" fontSize="14">x‚ÇÉ</text>
              
              {/* L√≠neas con pesos */}
              <line x1="70" y1="40" x2="150" y2="90" stroke="#a0a0b0" strokeWidth="2"/>
              <text x="100" y="55" fill="#f59e0b" fontSize="10">w‚ÇÅ</text>
              
              <line x1="70" y1="90" x2="150" y2="90" stroke="#a0a0b0" strokeWidth="2"/>
              <text x="100" y="82" fill="#f59e0b" fontSize="10">w‚ÇÇ</text>
              
              <line x1="70" y1="140" x2="150" y2="90" stroke="#a0a0b0" strokeWidth="2"/>
              <text x="100" y="125" fill="#f59e0b" fontSize="10">w‚ÇÉ</text>
              
              {/* Neurona central (suma) */}
              <circle cx="180" cy="90" r="30" fill="none" stroke="#00d4ff" strokeWidth="3"/>
              <text x="180" y="95" textAnchor="middle" fill="#00d4ff" fontSize="16">Œ£</text>
              
              {/* Bias */}
              <line x1="180" y1="150" x2="180" y2="120" stroke="#10b981" strokeWidth="2"/>
              <text x="180" y="165" textAnchor="middle" fill="#10b981" fontSize="12">+b</text>
              
              {/* Flecha a activaci√≥n */}
              <line x1="210" y1="90" x2="260" y2="90" stroke="#a0a0b0" strokeWidth="2" markerEnd="url(#arrowhead)"/>
              
              {/* Funci√≥n de activaci√≥n */}
              <rect x="260" y="65" width="60" height="50" rx="8" fill="none" stroke="#f59e0b" strokeWidth="2"/>
              <text x="290" y="85" textAnchor="middle" fill="#f59e0b" fontSize="10">Activaci√≥n</text>
              <text x="290" y="102" textAnchor="middle" fill="#f59e0b" fontSize="12">œÉ(z)</text>
              
              {/* Salida */}
              <line x1="320" y1="90" x2="360" y2="90" stroke="#a0a0b0" strokeWidth="2"/>
              <circle cx="375" cy="90" r="15" fill="none" stroke="#10b981" strokeWidth="2"/>
              <text x="375" y="95" textAnchor="middle" fill="#10b981" fontSize="12">y</text>
              
              {/* Arrowhead definition */}
              <defs>
                <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                  <polygon points="0 0, 10 3.5, 0 7" fill="#a0a0b0"/>
                </marker>
              </defs>
            </svg>
          </div>
        </div>

        <h4>Las Ecuaciones de una Neurona</h4>
        
        <p>Matem√°ticamente, una neurona hace dos operaciones:</p>
        
        <p><strong>1. Suma ponderada (producto escalar + sesgo):</strong></p>
        <div className="math-block">
          z = x‚ÇÅ¬∑w‚ÇÅ + x‚ÇÇ¬∑w‚ÇÇ + ... + x‚Çô¬∑w‚Çô + b = x‚Éó ¬∑ w‚Éó + b
        </div>
        
        <p><strong>2. Funci√≥n de activaci√≥n:</strong></p>
        <div className="math-block">
          y = œÉ(z)
        </div>
        
        <p>
          Donde œÉ puede ser ReLU, Sigmoid, Tanh, etc. La funci√≥n de activaci√≥n introduce 
          <strong> no-linealidad</strong>, lo que permite a la red aprender patrones complejos. 
          Sin ella, apilar capas ser√≠a equivalente a una sola capa lineal.
        </p>

        <h4>Implementaci√≥n en Python</h4>
        <CodeBlock
          title="producto_escalar_neurona.py"
          code={`import numpy as np

# Una neurona hace esto:
# output = activation(inputs ¬∑ weights + bias)

inputs = np.array([1.0, 2.0, 3.0])   # Entrada (vector de 3 features)
weights = np.array([0.5, -0.5, 0.3]) # Pesos de la neurona
bias = 0.1

# Paso 1: Suma ponderada (producto escalar + bias)
z = np.dot(inputs, weights) + bias
# z = 1.0*0.5 + 2.0*(-0.5) + 3.0*0.3 + 0.1
# z = 0.5 - 1.0 + 0.9 + 0.1 = 0.5

print(f"z = inputs ¬∑ weights + bias = {z}")

# Paso 2: Funci√≥n de activaci√≥n (ReLU)
def relu(x):
    return max(0, x)

output = relu(z)
print(f"output = ReLU(z) = {output}")`}
        />
      </section>

      {/* ========================================
          SECCI√ìN 4: GRADIENTES
          ======================================== */}
      <section id="gradientes" className="section">
        <h2>4. Gradientes y Descenso del Gradiente</h2>

        <p>
          El gradiente es simplemente la derivada pero para funciones de m√∫ltiples variables. 
          Es un <strong>vector</strong> que apunta en la direcci√≥n donde la funci√≥n crece m√°s r√°pido.
        </p>

        <div className="concept-box">
          <div className="concept-title">üéØ La Idea Central</div>
          <p style={{ margin: 0 }}>
            Si el gradiente te dice hacia d√≥nde &quot;sube&quot; la funci√≥n m√°s r√°pido, 
            entonces ir en direcci√≥n <strong>opuesta</strong> al gradiente te lleva 
            hacia donde la funci√≥n &quot;baja&quot; m√°s r√°pido. As√≠ encontramos el m√≠nimo 
            de la funci√≥n de p√©rdida.
          </p>
        </div>

        <div className="math-block">
          Para f(x, y): ‚àáf = [‚àÇf/‚àÇx, ‚àÇf/‚àÇy]
        </div>

        <h3>4.1 La Regla de Actualizaci√≥n (SGD)</h3>
        <p>
          El algoritmo de <strong>Descenso de Gradiente Estoc√°stico (SGD)</strong> actualiza 
          los par√°metros Œ∏ en cada iteraci√≥n siguiendo esta regla:
        </p>
        
        <div className="math-block">
          Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú - Œ± ¬∑ ‚àáL(Œ∏‚Çú)
        </div>
        
        <p>Donde:</p>
        <ul>
          <li><strong>Œ∏‚Çú</strong>: Par√°metros actuales (pesos de la red)</li>
          <li><strong>Œ±</strong>: Learning rate (tasa de aprendizaje) - qu√© tan grande es cada paso</li>
          <li><strong>‚àáL(Œ∏‚Çú)</strong>: Gradiente de la funci√≥n de p√©rdida respecto a los par√°metros</li>
          <li><strong>Œ∏‚Çú‚Çä‚ÇÅ</strong>: Nuevos par√°metros despu√©s de la actualizaci√≥n</li>
        </ul>

        <div className="concept-box">
          <div className="concept-title">‚ö†Ô∏è El signo menos es crucial</div>
          <p style={{ margin: 0 }}>
            <strong>Restamos</strong> el gradiente porque queremos ir en la direcci√≥n opuesta 
            al crecimiento. El gradiente apunta &quot;cuesta arriba&quot;; nosotros queremos ir 
            &quot;cuesta abajo&quot; hacia el m√≠nimo.
          </p>
        </div>

        <h3>4.2 Momentum</h3>
        <p>
          El problema de SGD b√°sico es que puede oscilar o quedarse atrapado. 
          <strong> Momentum</strong> a√±ade &quot;inercia&quot; - el algoritmo recuerda la direcci√≥n 
          en la que ven√≠a movi√©ndose.
        </p>

        <div className="math-block">
          v‚Çú = Œ≤ ¬∑ v‚Çú‚Çã‚ÇÅ + Œ± ¬∑ ‚àáL(Œ∏‚Çú)
        </div>
        <div className="math-block">
          Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú - v‚Çú
        </div>

        <p>Donde:</p>
        <ul>
          <li><strong>v‚Çú</strong>: Velocidad acumulada (como la velocidad de una bola rodando)</li>
          <li><strong>Œ≤</strong>: Coeficiente de momentum (t√≠picamente 0.9) - cu√°nto &quot;recuerda&quot;</li>
        </ul>

        <div className="deep-explanation">
          <h4>üé± Analog√≠a F√≠sica</h4>
          <p>
            Imagina una bola rodando por una colina. Sin momentum, la bola se para en cada 
            hoyo peque√±o. Con momentum, la bola acumula velocidad y puede &quot;saltar&quot; sobre 
            baches peque√±os para llegar al valle m√°s profundo.
          </p>
        </div>

        <h3>4.3 Simulaci√≥n Interactiva</h3>
        <p>
          Prueba la simulaci√≥n. Observa c√≥mo SGD puede zigzaguear mientras que 
          Momentum suaviza el camino. ¬°Genera nuevas superficies para ver diferentes casos!
        </p>

        <GradientDescentSimulation />

        <CodeBlock
          title="descenso_gradiente.py"
          code={`import numpy as np

def f(x, y):
    """Funci√≥n a minimizar: f(x, y) = x¬≤ + y¬≤"""
    return x**2 + y**2

def gradient_f(x, y):
    """Gradiente de f: ‚àáf = [2x, 2y]"""
    return np.array([2*x, 2*y])

# ===== SGD B√ÅSICO =====
learning_rate = 0.1
position = np.array([5.0, 5.0])

for i in range(50):
    grad = gradient_f(*position)
    position = position - learning_rate * grad  # Œ∏ = Œ∏ - Œ±‚àáL

# ===== CON MOMENTUM =====
position = np.array([5.0, 5.0])
velocity = np.array([0.0, 0.0])
beta = 0.9  # Coeficiente de momentum

for i in range(50):
    grad = gradient_f(*position)
    velocity = beta * velocity + learning_rate * grad  # v = Œ≤v + Œ±‚àáL
    position = position - velocity                      # Œ∏ = Œ∏ - v`}
        />

        <h3>4.4 Optimizadores Modernos</h3>
        <p>
          En la pr√°ctica, casi siempre usamos <strong>Adam</strong> que combina 
          momentum con learning rates adaptativos por par√°metro.
        </p>

        <CollapsibleSection title="Adam: El optimizador est√°ndar moderno" icon="üìñ">
          <p>
            <strong>Adam</strong> (Adaptive Moment Estimation) combina dos ideas:
          </p>
          <ol>
            <li>
              <strong>Momentum</strong>: Acumula una media m√≥vil de los gradientes pasados 
              (primer momento).
            </li>
            <li>
              <strong>RMSprop/AdaGrad</strong>: Adapta el learning rate para cada par√°metro 
              bas√°ndose en la magnitud de sus gradientes hist√≥ricos (segundo momento).
            </li>
          </ol>
          <p>
            Esto hace que Adam converja r√°pidamente y funcione bien &quot;out of the box&quot; 
            para la mayor√≠a de problemas. Es el optimizador por defecto en casi todos 
            los frameworks.
          </p>
          <CodeBlock
            title="usar_adam.py"
            code={`import torch.optim as optim

# Crear optimizador Adam
optimizer = optim.Adam(model.parameters(), lr=0.001)

# En el loop de entrenamiento:
optimizer.zero_grad()  # Limpiar gradientes
loss.backward()        # Calcular gradientes (backprop)
optimizer.step()       # Actualizar pesos con Adam`}
          />
        </CollapsibleSection>
      </section>

      {/* ========================================
          SECCI√ìN 5: MATRICES
          ======================================== */}
      <section id="matrices" className="section">
        <h2>5. Matrices y Redes Neuronales</h2>

        <p>
          Una matriz es una tabla rectangular de n√∫meros. En deep learning, las matrices 
          son fundamentales porque <strong>cada capa de una red neuronal es una multiplicaci√≥n 
          matricial</strong>.
        </p>

        <h3>5.1 ¬øQu√© hace realmente una neurona?</h3>
        <p>
          Antes de ver matrices, entendamos qu√© hace UNA neurona, y luego veremos c√≥mo 
          una capa entera es simplemente hacer muchas neuronas en paralelo (= multiplicaci√≥n matricial).
        </p>

        <div className="concept-box">
          <div className="concept-title">üß† Una Neurona = Producto Escalar + Activaci√≥n</div>
          <p style={{ margin: 0 }}>
            <code>output = activation(inputs ¬∑ weights + bias)</code>
            <br /><br />
            Es decir: toma las entradas, las multiplica por pesos, suma todo, a√±ade un sesgo, 
            y pasa el resultado por una funci√≥n no lineal.
          </p>
        </div>

        <h3>5.2 Una Capa = Muchas Neuronas = Multiplicaci√≥n Matricial</h3>
        <p>
          Si tenemos 3 entradas y queremos 4 neuronas, en lugar de hacer 4 productos escalares 
          separados, podemos hacer UNA multiplicaci√≥n matricial:
        </p>

        <CodeBlock
          title="capa_como_matriz.py"
          code={`import numpy as np

# Entrada: vector de 3 features
inputs = np.array([[1.0, 2.0, 3.0]])  # Shape: (1, 3) - un ejemplo

# Capa con 4 neuronas
# Cada columna de W son los pesos de una neurona
W = np.array([[0.1, 0.2, 0.3, 0.4],   # Pesos para input 1
              [0.5, 0.6, 0.7, 0.8],   # Pesos para input 2
              [0.9, 1.0, 1.1, 1.2]])  # Pesos para input 3
# Shape: (3, 4) - 3 inputs, 4 neuronas

bias = np.array([0.1, 0.1, 0.1, 0.1])  # Un bias por neurona

# La "magia": UNA multiplicaci√≥n matricial calcula las 4 neuronas
z = inputs @ W + bias  # Shape: (1, 4)
print(f"Salida de la capa: {z}")

# Aplicar activaci√≥n
output = np.maximum(0, z)  # ReLU
print(f"Despu√©s de ReLU: {output}")`}
        />

        <CollapsibleSection title="Visualizando el flujo de datos" icon="üìñ">
          <p>
            Pi√©nsalo as√≠:
          </p>
          <ul>
            <li><strong>Input</strong>: Un vector de N n√∫meros (features de un ejemplo).</li>
            <li><strong>Weights</strong>: Una matriz de N√óM donde M es el n√∫mero de neuronas.</li>
            <li><strong>Output</strong>: Un vector de M n√∫meros (uno por neurona).</li>
          </ul>
          <p>
            La multiplicaci√≥n matricial hace simult√°neamente los N productos escalares 
            que cada neurona necesita. Es por esto que las GPUs (dise√±adas para multiplicar 
            matrices r√°pidamente) son tan buenas para deep learning.
          </p>
          <p>
            Si tienes un batch de B ejemplos, tu input tiene shape (B, N) y la salida 
            tiene shape (B, M). ¬°Procesas B ejemplos en paralelo!
          </p>
        </CollapsibleSection>
      </section>

      {/* ========================================
          SECCI√ìN 6: PROBABILIDAD
          ======================================== */}
      <section id="probabilidad" className="section">
        <h2>6. Probabilidad y Estad√≠stica</h2>

        <p>
          La probabilidad es esencial para entender c√≥mo los modelos hacen predicciones 
          y c√≥mo medimos si esas predicciones son buenas o malas.
        </p>

        <h3>6.1 Distribuci√≥n de Probabilidad</h3>
        <p>
          Una distribuci√≥n asigna probabilidades a diferentes resultados posibles. 
          Todas las probabilidades deben sumar 1.
        </p>

        <CodeBlock
          title="distribucion_probabilidad.py"
          code={`import numpy as np

# Ejemplo: clasificaci√≥n de 3 clases (gato, perro, p√°jaro)
# La salida de softmax es una distribuci√≥n de probabilidad

logits = np.array([2.0, 1.0, 0.1])  # Salida cruda de la red

def softmax(x):
    exp_x = np.exp(x - np.max(x))  # Restar max para estabilidad
    return exp_x / exp_x.sum()

probabilidades = softmax(logits)

clases = ['gato', 'perro', 'p√°jaro']
for clase, prob in zip(clases, probabilidades):
    print(f"P({clase}) = {prob:.3f}")

print(f"\\nSuma = {probabilidades.sum():.3f}")  # Siempre 1.0
print(f"Predicci√≥n: {clases[np.argmax(probabilidades)]}")`}
        />

        <h3>6.2 Cross-Entropy Loss</h3>
        <p>
          Cross-entropy mide qu√© tan &quot;diferentes&quot; son dos distribuciones de probabilidad. 
          En clasificaci√≥n, comparamos la distribuci√≥n predicha con la distribuci√≥n real 
          (que es 100% en la clase correcta).
        </p>

        <div className="math-block">
          H(y, ≈∑) = -Œ£ y_i ¬∑ log(≈∑_i)
        </div>

        <CollapsibleSection title="Entendiendo Cross-Entropy intuitivamente" icon="üìñ">
          <p>
            Supongamos que la clase correcta es &quot;gato&quot; (clase 0):
          </p>
          <ul>
            <li>Etiqueta real (one-hot): y = [1, 0, 0]</li>
            <li>Predicci√≥n buena: ≈∑ = [0.9, 0.05, 0.05]</li>
            <li>Predicci√≥n mala: ≈∑ = [0.1, 0.5, 0.4]</li>
          </ul>
          <p>
            Como y_i = 1 solo para la clase correcta (gato), la f√≥rmula se simplifica a:
          </p>
          <div className="math-block">
            H = -log(≈∑_gato)
          </div>
          <ul>
            <li>Predicci√≥n buena: H = -log(0.9) ‚âà 0.105 (loss bajo = bueno)</li>
            <li>Predicci√≥n mala: H = -log(0.1) ‚âà 2.303 (loss alto = malo)</li>
          </ul>
          <p>
            Cuanto m√°s seguro est√° el modelo de la respuesta correcta (≈∑ cercano a 1), 
            menor es el loss. Si el modelo asigna probabilidad casi 0 a la respuesta correcta, 
            el loss explota ‚Üí gran correcci√≥n en backprop.
          </p>
        </CollapsibleSection>

        <h3>6.3 Esperanza Matem√°tica (Valor Esperado)</h3>
        <p>
          El valor esperado es el &quot;promedio ponderado&quot; de todos los resultados posibles.
        </p>

        <div className="math-block">
          E[X] = Œ£ x_i ¬∑ P(x_i)
        </div>

        <CodeBlock
          title="esperanza_matematica.py"
          code={`import numpy as np

# Ejemplo: Valor esperado de un dado de 6 caras
caras = np.array([1, 2, 3, 4, 5, 6])
probs = np.array([1/6] * 6)  # Cada cara tiene prob 1/6

esperanza = np.sum(caras * probs)

print(f"E[dado] = {esperanza:.2f}")
# = (1 + 2 + 3 + 4 + 5 + 6) / 6 = 3.5

# Nota: 3.5 no es un resultado posible,
# pero es el promedio a largo plazo si tiras muchas veces`}
        />

        <h3>6.4 Probabilidad Condicional</h3>
        <p>
          P(A|B) es la probabilidad de A <strong>dado que ya sabemos</strong> que B ocurri√≥.
        </p>

        <div className="math-block">
          P(A|B) = P(A ‚à© B) / P(B)
        </div>

        <CollapsibleSection title="Ejemplo pr√°ctico" icon="üìñ">
          <p>
            Lanzamos un dado. Sea:
          </p>
          <ul>
            <li>A = obtener un 6</li>
            <li>B = obtener un n√∫mero par (2, 4, o 6)</li>
          </ul>
          <p>
            ¬øCu√°l es P(6 | par)?
          </p>
          <ul>
            <li>P(A ‚à© B) = P(6 y par) = 1/6 (el √∫nico resultado que es 6 y par es el 6)</li>
            <li>P(B) = P(par) = 3/6 = 0.5</li>
            <li>P(6 | par) = (1/6) / (0.5) = 1/3 ‚âà 0.33</li>
          </ul>
          <p>
            Tiene sentido: si ya sabemos que es par, solo puede ser 2, 4 o 6. 
            La probabilidad de que sea espec√≠ficamente 6 es 1 de 3.
          </p>
          <p>
            <strong>En NLP</strong>: Los modelos de lenguaje calculan P(siguiente_palabra | palabras_anteriores). 
            ¬°Es probabilidad condicional pura!
          </p>
        </CollapsibleSection>
      </section>

      {/* Navegaci√≥n */}
      <div style={{ 
        display: 'flex', 
        justifyContent: 'space-between', 
        marginTop: '4rem',
        paddingTop: '2rem',
        borderTop: '1px solid var(--border)'
      }}>
        <Link href="/" className="btn btn-secondary">
          ‚Üê Volver al Inicio
        </Link>
        <Link href="/pytorch-fundamentals" className="btn btn-primary">
          Siguiente: PyTorch ‚Üí
        </Link>
      </div>
    </div>
  );
}
