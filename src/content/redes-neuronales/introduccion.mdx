---
title: "El Perceptr贸n y Backpropagation"
order: 1
description: "Construyendo la arquitectura fundamental del Deep Learning"
---

#  Redes Neuronales

El puente entre la matem谩tica y la inteligencia artificial. Aqu铆 aprender谩s a construir la arquitectura fundamental que permite a las m谩quinas aprender patrones complejos: el **Perceptr贸n Multicapa**.

<div className="concept-box mb-12">
### Filosof铆a "De Cero"
No usaremos `torch.nn` hasta que hayamos implementado manualmente el paso hacia adelante (forward) y hayamos entendido la mec谩nica del gradiente.
</div>

## 1. La Neurona Artificial

Inspirada en la biolog铆a, una neurona artificial es simplemente una funci贸n matem谩tica que suma entradas ponderadas y decide si "disparar" o no mediante una activaci贸n.

<Mermaid>
graph LR
    subgraph Entrada
    X1[x1]
    X2[x2]
    X3[x3]
    end

    subgraph CuerpoCelular
    Sum[Suma]
    Act[Activacion]
    end

    Out[y]

    X1 --> Sum
    X2 --> Sum
    X3 --> Sum
    Sum --> Act
    Act --> Out
</Mermaid>

<div className="card bg-slate-900/40 p-6 border-slate-700">
   <h4 className="mt-0 text-sm uppercase tracking-widest text-[var(--text-secondary)]">La Ecuaci贸n Sagrada</h4>
   $$ y = \sigma(\sum_i w_i x_i + b) $$
   <div className="text-sm italic mb-0">Donde $w$ son los pesos, $x$ las entradas, $b$ el sesgo y $\sigma$ la activaci贸n.</div>
</div>

<CodeBlock
  title="neurona_manual.py"
  language="python"
  code={`import torch

def simple_neuron(x, w, b):
    # Suma ponderada + sesgo
    z = torch.dot(x, w) + b
    # Activaci贸n ReLU
    return torch.max(torch.tensor(0.0), z)

x = torch.tensor([1.2, 0.5, -1.0])
w = torch.tensor([0.1, -0.8, 0.5])
b = torch.tensor(0.1)

print(f"Salida: {simple_neuron(x, w, b)}")`}
/>

## 2. Backpropagation

<CollapsibleSection title="驴C贸mo aprende realmente la red?" icon="">
No es magia, es **c谩lculo**. Backpropagation es el proceso de usar la regla de la cadena para calcular el gradiente de la funci贸n de p茅rdida respecto a cada peso. 
</CollapsibleSection>

<Mermaid>
graph RL
    L[Loss] --> P[Prediccion]
    P --> H[Capas Ocultas]
    H --> I[Entrada]
    
    subgraph Aprendizaje
    direction RL
    L -.-> P
    P -.-> H
    end
</Mermaid>

<div className="grid grid-cols-1 md:grid-cols-2 gap-8 my-12 text-balance">
   <div className="card border-l-4 border-l-[var(--highlight)] p-6 mb-0 bg-slate-900/20">
      <h4 className="mt-0">1. Forward Pass</h4>
      <div className="text-sm">Pasamos los datos y calculamos la predicci贸n y el error (Loss).</div>
   </div>
   <div className="card border-l-4 border-l-[var(--highlight-secondary)] p-6 mb-0 bg-slate-900/20">
      <h4 className="mt-0">2. Backward Pass</h4>
      <div className="text-sm">Viajamos hacia atr谩s calculando derivadas para saber qu茅 pesos son los culpables del error.</div>
   </div>
</div>

<CodeBlock
  title="entrenamiento_autograd.py"
  language="python"
  code={`# PyTorch hace el trabajo sucio con Autograd
weights = torch.randn(3, requires_grad=True)
loss = (output - target) ** 2

loss.backward() # 隆Magia! Calcula todos los gradientes
print(weights.grad) # Gradiente acumulado`}
/>
