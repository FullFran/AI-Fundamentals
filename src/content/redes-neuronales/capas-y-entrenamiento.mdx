---
title: "Construyendo un MLP"
order: 2
description: "De la neurona a la red multicapa completa"
---

# üèóÔ∏è Construyendo un MLP

Un Multi-Layer Perceptron (MLP) es simplemente una pila de capas lineales separadas por funciones de activaci√≥n.

## 1. La Capa Lineal (Linear Layer)

Implementamos una capa que soporta m√∫ltiples neuronas y almacenamiento de resultados para backprop.

<CodeBlock
  title="capa_densa.py"
  language="python"
  code={`import torch

class LinearLayer:
    def __init__(self, n_inputs, n_neurons):
        # Escalar pesos para evitar gradientes explosivos
        self.weights = 0.01 * torch.randn(n_inputs, n_neurons)
        self.bias = torch.zeros(1, n_neurons)
        
    def forward(self, inputs):
        self.output = torch.matmul(inputs, self.weights) + self.bias
        return self.output`}
/>

## 2. Uniendo todo: Red Neuronal Simple

Usando `torch.nn.Module`, la clase base de PyTorch para todas las redes.

<CodeBlock
  title="red_completa.py"
  language="python"
  code={`import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(2, 4) # 2 entradas -> 4 ocultas
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(4, 1) # 4 ocultas -> 1 salida
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        x = self.layer1(x)
        x = self.relu(x)
        x = self.layer2(x)
        x = self.sigmoid(x)
        return x

model = SimpleNet()
input_data = torch.randn(5, 2) # Batch de 5 ejemplos
output = model(input_data)
print(output)`}
/>

<concept-box>
### Loop de Entrenamiento
El ciclo est√°ndar es:
1. **Forward**: Calcular predicci√≥n.
2. **Loss**: Calcular error.
3. **Backward**: Calcular gradientes.
4. **Step**: Actualizar pesos.
</concept-box>
