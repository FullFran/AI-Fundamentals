---
title: "Mezcla de Expertos (MoE)"
order: 3
description: "Escalar modelos sin multiplicar el coste en cada paso"
---

# üß© Mezcla de Expertos (MoE)

Aqu√≠ es donde el escalado se vuelve inteligente. En vez de usar **todas** las neuronas para **todas** las entradas, MoE decide **qui√©n trabaja** en cada token. Es como un hospital: no llamas a todos los m√©dicos a la vez, llamas al especialista adecuado.

## 1. La idea base
Un MoE tiene dos partes:
* **Expertos:** peque√±os modelos especializados.
* **Router (Gate):** decide qu√© expertos reciben cada entrada.

Resultado: m√°s capacidad total, pero coste por token controlado.

## 2. ¬øQu√© hace el Router?
El router calcula una puntuaci√≥n por experto y activa solo unos pocos (top-k). Esto introduce **sparsity**: solo una parte del modelo trabaja en cada paso.

<CodeBlock
  title="router_moe.py"
  language="python"
  code={`import torch
import torch.nn as nn

class MoERouter(nn.Module):
    def __init__(self, d_model, n_experts, k=2):
        super().__init__()
        self.gate = nn.Linear(d_model, n_experts)
        self.k = k

    def forward(self, x):
        # x: (batch, seq, d_model)
        scores = self.gate(x)
        topk = torch.topk(scores, self.k, dim=-1)
        return topk.indices, topk.values
`}
/>

## 3. Ventajas y costes reales
* **Ventaja:** escalas parametros sin escalar el coste por token.
* **Coste oculto:** balancear carga entre expertos y evitar que uno lo haga todo.

<div className="deep-explanation">
**F√≠sica mental:** es como repartir energia entre canales paralelos para no saturar uno solo. Si un experto se calienta demasiado, el sistema pierde estabilidad.
</div>

## 4. Referencias clave
* **Shazeer et al.** *Outrageously Large Neural Networks* (2017).
* **Lepikhin et al.** *GShard* (2020).
* **Fedus et al.** *Switch Transformers* (2021).
