---
title: "Probabilidad y Estad铆stica"
order: 4
description: "Midiendo la incertidumbre en las predicciones"
---

#  Probabilidad y Estad铆stica

La probabilidad nos permite entender c贸mo los modelos hacen predicciones y c贸mo medimos qu茅 tan buenas son.

## Distribuci贸n de Probabilidad

Una distribuci贸n asigna probabilidades a resultados posibles. En clasificaci贸n, la salida de una red (despu茅s de Softmax) es una distribuci贸n.

<CodeBlock
  title="softmax_dist.py"
  language="python"
  code={`import numpy as np

def softmax(x):
    exp_x = np.exp(x - np.max(x))
    return exp_x / exp_x.sum()

logits = np.array([2.0, 1.0, 0.1]) # Scores crudos
probabilidades = softmax(logits)

print(f"Probabilidades: {probabilidades}")
print(f"Suma total: {probabilidades.sum()}") # Siempre 1.0`}
/>

## Cross-Entropy Loss

Esta es la m茅trica reina en clasificaci贸n. Mide qu茅 tan "diferente" es la predicci贸n del modelo de la etiqueta real.

<math-block>
  H(y, 欧) = -危 y_i 路 log(欧_i)
</math-block>

<CollapsibleSection title="Intuici贸n de Cross-Entropy">
Si el modelo est谩 muy seguro de la clase correcta (probabilidad cercana a 1), el logaritmo es cercano a 0 y la p茅rdida es m铆nima. Si est谩 seguro de la clase equivocada, la p茅rdida explota.
</CollapsibleSection>

## Probabilidad Condicional en NLP

Los modelos de lenguaje (LLMs) calculan la probabilidad de la siguiente palabra dadas las anteriores.
