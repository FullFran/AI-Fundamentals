---
title: "01. L칩gica, Conjuntos y Funciones"
order: 1
description: "Fundamentos formales: de la teor칤a de conjuntos a la topolog칤a de funciones."
---

# 游빑 L칩gica, Conjuntos y Funciones

Mira, vamos a ser claros: las matem치ticas no son un castigo, son **el lenguaje secreto de la realidad**. Si quieres construir algo que piense, primero tienes que entender c칩mo se estructura el pensamiento en el universo. Antes de tocar una sola l칤nea de c칩digo en PyTorch, tenemos que entender c칩mo las funciones transforman los datos. Es as칤 de f치cil, y a la vez, es una locura.

## 1. 쯈u칠 es, de verdad, una Funci칩n?
Olvida las gr치ficas aburridas del instituto. Una funci칩n $f: A \to B$ es una **m치quina de transformar mundos**. Tomas algo de un sitio (el dominio $A$) y lo lanzas a otro (el codominio $B$).

En Deep Learning, casi todo lo que hacemos es construir funciones gigantescas. Una imagen de un gato entra por un lado (miles de n칰meros) y sale una sola palabra por el otro: "Gato".

<InteractiveGraph type="linear" title="Funci칩n Lineal: La base de todo" />

### Las reglas del juego
Para que esto funcione en serio, necesitamos rigor. No nos vale cualquier "m치quina":
*   **Inyectividad:** Queremos que nuestra m치quina no sea "tonta". Si le das dos cosas distintas, lo ideal es que te devuelva dos resultados distintos. Si no, est치s perdiendo informaci칩n por el camino.
*   **Continuidad:** Esta es la clave de todo el Deep Learning. $\forall \epsilon > 0 \dots$ ya sabes el rollo formal, pero lo que significa de verdad es: **"Si cambio un poquito la entrada, el resultado cambia un poquito"**. Si tu red no es continua, el aprendizaje se rompe. Es as칤 de sencillo.

## 2. La Magia de la Sigmoide
Ostras, esta funci칩n es una maravilla. Es la que nos permite pasar del mundo infinito de los n칰meros reales al mundo acotado de las probabilidades (entre 0 y 1).

<InteractiveGraph type="sigmoid" title="Sigmoide: Comprimiendo el infinito" />

<CodeBlock
  title="sigmoide_basica.py"
  language="python"
  code={`import numpy as np

# Esta peque침a funcion es la que encendio la mecha de las redes neuronales
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

print(f"En el centro, todo es duda: {sigmoid(0)}") # 0.5`}
/>

## 3. Espacios M칠tricos y Distancias
Un conjunto $M$ con una funci칩n de distancia (m칠trica) $d(x, y)$ que cumple con la desigualdad triangular. Esto es la base de las funciones de p칠rdida (**Loss Functions**). Si no sabemos medir la distancia entre la predicci칩n y la realidad, el modelo no puede aprender nada.

## 4. Referencias Acad칠micas
*   **Michael Spivak**, *Calculus*. El libro sagrado para entender el rigor del an치lisis matem치tico.
*   **Walter Rudin**, *Principles of Mathematical Analysis*. Para aquellos que quieran profundizar en la topolog칤a de los espacios reales.
*   **MIT OCW 18.01**, *Single Variable Calculus*. Bases formales y visuales del c치lculo.
*   **MIT OCW 18.02**, *Multivariable Calculus*. Extiende la intuici칩n a dimensiones altas.
{/* Apuntes internos: llm-ref/notas/semana_1_notas_init.md */}

<div className="concept-box">
**Ejercicio Pro:** Demuestra que la funci칩n ReLU $f(x) = \max(0, x)$ es continua pero no diferenciable en $x=0$. 쮺칩mo afecta esto al descenso del gradiente?
</div>
