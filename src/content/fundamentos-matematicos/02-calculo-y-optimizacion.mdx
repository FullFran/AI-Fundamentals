---
title: "02. C치lculo y Optimizaci칩n Multivariable"
order: 2
description: "Gradientes, Jacobianos y la geometr칤a de la optimizaci칩n."
---

# 游늻 C치lculo y Optimizaci칩n

Yo lo veo as칤: entrenar un modelo es como soltar una canica en un paisaje de energ칤a y ver c칩mo encuentra el valle m치s profundo. En IA, ese paisaje se llama funci칩n de coste $J(\theta)$. Lo que buscamos es el conjunto de par치metros $\theta$ que la minimiza.

## 1. El Gradiente ($\nabla$)
Para una funci칩n escalar $f: \mathbb{R}^n \to \mathbb{R}$, el gradiente es el vector de sus derivadas parciales:
$$\nabla f(x) = \left[ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n} \right]^T$$

**Intuici칩n:** El gradiente apunta en la direcci칩n de m치ximo crecimiento local. Por eso, para minimizar, nos movemos en la direcci칩n $-\nabla f(x)$.

<div className="concept-box">
**Conexi칩n F칤sica:** El gradiente es la fuerza que siente una part칤cula en un potencial. Si el potencial es $E(x)$, la fuerza apunta a $-\nabla E(x)$. Entrenar una red es literalmente dejar que el sistema baje energ칤a.
</div>

## 2. Matriz Hessiana y Convexidad
La Hessiana $H$ contiene las segundas derivadas: $H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}$.
*   Si $H$ es definida positiva en todo el dominio, la funci칩n es **convexa**.
*   En funciones convexas, cualquier m칤nimo local es un m칤nimo global. 춰El para칤so de la optimizaci칩n!
*   **Realidad en IA:** Las redes neuronales son NO convexas. Tenemos que lidiar con puntos de silla y m칤nimos locales.

## 3. Jacobianos y Backpropagation
Cuando tenemos funciones vectoriales $f: \mathbb{R}^n \to \mathbb{R}^m$, usamos la matriz Jacobiana. La **regla de la cadena** multivariable es lo que permite que el error fluya hacia atr치s en una red (Backpropagation).

## 4. Referencias Acad칠micas
*   **Stephen Boyd & Lieven Vandenberghe**, *Convex Optimization*. Referencia absoluta en ingenier칤a.
*   **Gilbert Strang**, *Calculus*. Excelente para construir la intuici칩n geom칠trica antes del rigor.
*   **MIT OCW 18.02**, *Multivariable Calculus*. Gradientes, Jacobianos y geometr칤a en alta dimensi칩n.
*   **MIT OCW 18.06**, *Linear Algebra*. El soporte algebraico detr치s de la optimizaci칩n.
{/* Apuntes internos: llm-ref/notas/semana_3_notas.md */}

<div className="deep-explanation">
**쯇or qu칠 el segundo orden importa?**
Aunque usamos primer orden (Gradiente), el condicionamiento de la Hessiana determina qu칠 tan r치pido podemos converger. Un radio de curvatura muy dispar hace que el optimizador "oscile".
</div>
