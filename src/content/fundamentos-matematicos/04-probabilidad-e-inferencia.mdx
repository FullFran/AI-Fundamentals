---
title: "04. Probabilidad e Inferencia"
order: 4
description: "Modelando la incertidumbre: de Bayes a la Verosimilitud."
---

#  Probabilidad e Inferencia

Si el mundo fuera determinista, la IA seria aburrida. Pero no lo es. El mundo es ruidoso, impredecible, y la probabilidad es la forma correcta de hablar de ese ruido sin mentirnos.

## 1. Probabilidad Bayesiana vs Frecuentista
*   **Frecuentista:** La probabilidad es el l铆mite de la frecuencia relativa.
*   **Bayesiana:** La probabilidad es un grado de creencia que se actualiza con datos.
$$P(\theta | D) = \frac{P(D | \theta) P(\theta)}{P(D)}$$
Esta f贸rmula es el coraz贸n de la IA moderna (MAP, VAEs, Difusi贸n).

## 2. Variables Aleatorias y Distribuciones
No basta con saber la media. Necesitamos entender la **varianza**, la **entrop铆a** y la **divergencia de Kullback-Leibler (KL)**. La divergencia KL mide qu茅 tan "lejos" est谩 una distribuci贸n de otra, clave en el entrenamiento de modelos generativos.

<div className="concept-box">
**Conexi贸n F铆sica:** La entropia mide el desorden. En termodinamica, eso decide hacia donde va un sistema. En IA, la entropia decide si un modelo se colapsa o explora. Es la misma pelea, con otras palabras.
</div>

## 3. Estimaci贸n por M谩xima Verosimilitud (MLE)
Entrenar una red neuronal con una p茅rdida de Entrop铆a Cruzada (Cross-Entropy) es equivalente a realizar MLE sobre una distribuci贸n de Bernoulli o Categ贸rica. Estamos buscando los par谩metros que hacen que los datos observados sean lo m谩s probables posible.

## 4. Referencias Acad茅micas
*   **E.T. Jaynes**, *Probability Theory: The Logic of Science*. Para una visi贸n profunda de la probabilidad como l贸gica.
*   **Christopher Bishop**, *Pattern Recognition and Machine Learning*. El cap铆tulo 1 y 2 son esenciales.
*   **MIT OCW 6.041**, *Probabilistic Systems Analysis and Applied Probability*.
*   **MIT OCW 18.05**, *Introduction to Probability and Statistics*.
{/* Apuntes internos: llm-ref/notas/semana_4_notas.md */}

<div className="deep-explanation">
**El Teorema Central del L铆mite:** Explica por qu茅 el ruido en los datos a menudo sigue una distribuci贸n Normal (Gaussiana) y por qu茅 las inicializaciones de pesos suelen basarse en distribuciones normales.
</div>
