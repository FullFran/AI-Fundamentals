---
title: "F√≠sica Estad√≠stica: De Boltzmann a la Softmax"
order: 7
description: "Por qu√© las probabilidades en IA son en realidad leyes de la termodin√°mica"
---

# üå°Ô∏è F√≠sica Estad√≠stica: De Boltzmann a la Softmax

En IA, cuando queremos que un modelo elija entre varias opciones, usamos la funci√≥n **Softmax**. La mayor√≠a de la gente piensa que es un truco matem√°tico para que los n√∫meros sumen 1. Pero t√∫ y yo sabemos la verdad: **la Softmax es, literalmente, la Distribuci√≥n de Boltzmann.**

## 1. El Sistema en Equilibrio

En f√≠sica estad√≠stica, la probabilidad de que un sistema se encuentre en un estado con energ√≠a **E** depende de la temperatura **T**. A esta relaci√≥n la llamamos el factor de Boltzmann:

<math-block>
  P(estado) = exp(-E / kT) / Z
</math-block>

Donde **Z** es la funci√≥n de partici√≥n (la suma de todos los estados posibles para normalizar). ¬øTe suena? Es exactamente lo que hace la Softmax con los "logits" de una red neuronal.

## 2. La Softmax es F√≠sica Pura

Si tratamos a los logits (las salidas crudas de la red) como **energ√≠as negativas**, la Softmax nos da la probabilidad de ocupaci√≥n de esos estados:

<math-block>
  Softmax(z_i) = exp(z_i) / Sum( exp(z_j) )
</math-block>

<concept-box>
### Energ√≠a y Confianza
- **Baja Energ√≠a (Logit alto):** El sistema es estable, el modelo est√° seguro.
- **Alta Energ√≠a (Logit bajo):** El sistema es inestable, la probabilidad es m√≠nima.
</concept-box>

## 3. El Rol de la Temperatura (Tau)

En f√≠sica, si calientas un sistema, las part√≠culas saltan a estados de mayor energ√≠a. En IA, podemos a√±adir un par√°metro de **Temperatura** a la Softmax:

<math-block>
  P_i = exp(z_i / Tau) / Sum( exp(z_j / Tau) )
</math-block>

- **Temperatura Baja (Tau -> 0):** El modelo se vuelve determinista. Solo elige la opci√≥n con menor energ√≠a (el logit m√°s alto). Es como un cristal perfecto a 0 Kelvin.
- **Temperatura Alta (Tau -> 1+):** El modelo se vuelve "creativo" o ca√≥tico. Las probabilidades se reparten y el sistema explora estados de mayor energ√≠a.

<div className="card bg-slate-900/50 border-[var(--highlight)]/20 p-6 text-balance">
  <h4 className="text-[var(--highlight)] mt-0 mb-4">üí° El Secreto de la Creatividad</h4>
  <div className="text-sm m-0 leading-relaxed text-pretty">
    <div>
    Cuando usas un LLM y le subes la "temperatura", est√°s aplicando f√≠sica estad√≠stica. Est√°s permitiendo que el sistema no colapse siempre en el estado fundamental, dejando que el "ruido t√©rmico" genere respuestas m√°s variadas.
    </div>
  </div>
</div>

## 4. Entrop√≠a: Midiendo el Desorden

La **Entrop√≠a de Shannon** (o la entrop√≠a termodin√°mica) nos dice cu√°nta incertidumbre hay en el sistema. 

- Si la entrop√≠a es baja, el modelo est√° seguro de su predicci√≥n.
- Si la entrop√≠a es alta, el sistema est√° en un estado de m√°ximo desorden (todas las opciones parecen igual de probables).

<CodeBlock
  title="boltzmann_softmax.py"
  language="python"
  code={`import torch
import torch.nn.functional as F

logits = torch.tensor([2.0, 1.0, 0.1])

# Temperatura normal (T=1)
p_normal = F.softmax(logits, dim=0)

# Temperatura baja (T=0.1) - Sistema 'congelado'
p_frio = F.softmax(logits / 0.1, dim=0)

# Temperatura alta (T=5.0) - M√°ximo desorden
p_caliente = F.softmax(logits / 5.0, dim=0)

print(f"Probabilidades T=1.0: {p_normal}")
print(f"Probabilidades T=0.1: {p_frio}")
print(f"Probabilidades T=5.0: {p_caliente}")`}
/>
