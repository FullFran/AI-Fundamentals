---
title: "√Ålgebra Lineal: Geometr√≠a y Operadores"
order: 3
description: "Vectores, transformaciones y el esqueleto geom√©trico del Deep Learning"
---

# üìê √Ålgebra Lineal: El Espacio de la Inteligencia

Esc√∫chame bien: el √°lgebra lineal **no se trata de tablas de n√∫meros**. Eso es para las m√°quinas. Para nosotros, se trata de **GEOMETR√çA**. En IA, los datos no son puntos perdidos; son flechas en un espacio gigante, y las redes neuronales son los "operadores" que deforman ese espacio para encontrar la verdad.

<concept-box>
### El Vector como Flecha
Un vector representa un **estado de informaci√≥n**. Un "embedding" de una palabra es un vector en un espacio donde la cercan√≠a geom√©trica significa que las cosas se parecen. Si dos flechas apuntan al mismo lado, est√°n hablando de lo mismo.
</concept-box>

## 1. El Operador: Una Matriz que Deforma
Una matriz **W** en una red neuronal es un **operador lineal**. No pienses en "multiplicar", piensa en "transformar". 

Cuando tus datos pasan por una capa, el espacio hace tres cosas:
1. **Rota:** Alinea la informaci√≥n con nuevas direcciones de inter√©s.
2. **Escala:** Le da volumen a lo importante y achica lo que no sirve.
3. **Proyecta:** Aplasta dimensiones para quedarse con la esencia. Es as√≠ de simple.

<Mermaid chart="
graph LR
    A[Espacio V] --> B[Operador W]
    B --> C[Espacio W]
    
    subgraph Deformacion
    direction TB
    D[Rotacion]
    E[Escalado]
    F[Proyeccion]
    end
    
    style A fill:#1e293b,stroke:#00d4ff
    style B fill:#00d4ff10,stroke:#00d4ff
    style C fill:#1e293b,stroke:#7c3aed
    style Deformacion fill:#1a1a3a,stroke:#334155
" />

## 2. El Producto Escalar: Medir la Alineaci√≥n
El producto escalar mide cu√°nto se "alinean" dos flechas. Es el coraz√≥n de los Transformers.

<math-block>
  a ¬∑ b = |a| |b| cos(theta)
</math-block>

### Intuici√≥n de la Atenci√≥n
En los Transformers, tienes una **Query (Q)** y una **Key (K)**. La red proyecta una sobre la otra. 
- Si est√°n alineadas (0 grados), la red "presta atenci√≥n".
- Si son ortogonales (90 grados), a la red no le importa. 

Es como un filtro de polarizaci√≥n: solo pasa la luz que est√° alineada. ¬°Es pura f√≠sica!

## 3. Autovectores: El Esqueleto del Espacio
Casi todas las flechas cambian de direcci√≥n cuando aplicas una matriz, pero hay unas flechas "sagradas": los **Autovectores**. Cuando las transformas, mantienen su direcci√≥n, solo cambian de tama√±o (seg√∫n su **Autovalor**).

<div className="card bg-slate-900/50 border-[var(--highlight)]/20 p-6 text-balance">
  <h4 className="text-[var(--highlight)] mt-0 mb-4">¬øPor qu√© te importa esto?</h4>
  <div className="text-sm m-0 leading-relaxed text-pretty">
    Los autovectores son las **caracter√≠sticas fundamentales** que la red est√° detectando. Si una red clasifica fotos de gatos, sus autovectores principales representan las formas b√°sicas (orejas, bigotes, ojos). El autovalor te dice qu√© tan importante es esa caracter√≠stica. 
  </div>
</div>

## 4. SVD: Encontrar la Se√±al en el Ruido
La **Descomposici√≥n en Valores Singulares (SVD)** permite romper cualquier matriz en sus componentes esenciales. Es lo que nos permite comprimir modelos gigantes. Si tiramos los componentes con valores singulares chiquitos, el modelo sigue funcionando pero pesa la mitad. ¬°Es magia geom√©trica!

<CodeBlock
  title="deformacion_espacial.py"
  language="python"
  code={`import numpy as np

# Nuestro vector (una flecha apuntando a la derecha)
x = np.array([1, 0]) 

# Operador que rota 90 grados y duplica el tama√±o
W = np.array([[0, -2],
              [2,  0]])

# Aplicamos la transformaci√≥n
x_transformed = W @ x

print(f"Original: {x}")
print(f"Transformado: {x_transformed}") # Flecha apuntando arriba con tama√±o 2`}
/>
