---
title: "06. Sistemas Din치micos y F칤sica Estad칤stica"
order: 6
description: "De la Memoria Asociativa al Paisaje de Energ칤a y Softmax."
---

# 游 Sistemas Din치micos y Energ칤a

쯊e has preguntado alguna vez por qu칠 una red neuronal puede "recordar" algo? Para entenderlo, tenemos que dejar de pensar como programadores y empezar a pensar como f칤sicos. Vamos a mirar c칩mo se comporta la materia para entender c칩mo se comporta la inteligencia.

Ya te digo yo que esto es lo m치s bonito de todo el curso: el **Modelo de Ising**. Es la historia de c칩mo peque침as unidades individuales (como 치tomos o neuronas) se ponen de acuerdo para crear un orden colectivo a partir del caos.

## 1. El Paisaje de Energ칤a: Esculpiendo recuerdos
Imagina que el conocimiento es un mapa lleno de monta침as y valles. Cuando un sistema f칤sico busca la estabilidad, busca la **m칤nima energ칤a**. Se deja caer al fondo del valle.

En IA, aprender no es "guardar datos en un disco duro". Aprender es **moldear ese paisaje**. Est치s usando el descenso del gradiente para cavar pozos en el mapa de energ칤a de modo que, cuando le des a la red una imagen ruidosa, el sistema "caiga" de forma natural hacia el recuerdo correcto. 춰Es una locura!

$$ E = - \sum_{i,j} J_{ij} s_i s_j $$

## 2. La Conexi칩n con Hopfield
John Hopfield se dio cuenta de algo que cambi칩 la historia: si tratamos a las neuronas como espines de un im치n, podemos usar la f칤sica para crear memoria.

<div className="concept-box">
### El Hamiltoniano: La partitura del sistema
En una Red de Hopfield, la energ칤a total del sistema es nuestra gu칤a:
$$ H = -\frac{1}{2} \sum_{i,j} w_{ij} s_i s_j $$
Recuperar un recuerdo es simplemente dejar que las neuronas "se relajen" y encuentren el estado de m칤nima energ칤a. Es como ver una canica rodar hasta el fondo de un cuenco.
</div>

## 3. El Salto a la Atenci칩n Moderna
Aqu칤 viene el puente que faltaba. En 2020 se public칩 **"Hopfield Networks is All You Need"**, y mostr칩 algo precioso: la atenci칩n moderna puede verse como una **Hopfield continua**. En lugar de memorias discretas, tienes **patrones continuos** en un espacio de embeddings. El mecanismo de atenci칩n no es magia: es recuperaci칩n de memoria, pero en alta dimensi칩n y de forma diferenciable.

쯃a idea clave? La atenci칩n calcula similitudes (Q췅K), usa softmax para convertirlas en pesos y recupera un vector como combinaci칩n de valores. Eso es exactamente lo que hace una Hopfield: buscar el atractor m치s compatible con la se침al de entrada, solo que aqu칤 lo hacemos en un solo paso y de forma paralela.

## 4. Atractores, Softmax y Temperatura
En f칤sica, un **atractor** es ese punto del valle que "atrae" todo lo que cae cerca. 
*   쯏 la famosa funci칩n **Softmax**? No es m치s que una distribuci칩n de Boltzmann disfrazada:
$$P(x) = \frac{1}{Z} e^{-E(x)/T}$$

Aqu칤 entra en juego la **Temperatura ($T$)**. Si la temperatura es alta, hay mucho ruido y el sistema salta de un valle a otro. Si es baja, el sistema se queda "congelado" en el m칤nimo. Por eso en los modelos de lenguaje (LLMs) ajustamos la "temperatura" para que sean m치s creativos o m치s deterministas. 춰Es f칤sica pura!

<div className="deep-explanation">
**Nota honesta:** Softmax no nacio de la fisica. Viene de regresion logistica y modelos probabilisticos. La lectura de Boltzmann es una interpretacion preciosa porque conecta energia y entropia, pero es a posteriori. Eso no la hace menos util; la hace mas profunda.
</div>


## 5. Referencias Acad칠micas
*   **Marc M칠zard & Andrea Montanari**, *Information, Physics, and Computation*. La biblia de esta conexi칩n.
*   **John Hopfield**, *Neural networks and physical systems with emergent collective computational abilities* (1982).
*   **Ramsauer et al.**, *Hopfield Networks is All You Need* (2020).
*   **MIT OCW 8.044**, *Statistical Physics I*.
{/* Apuntes internos: llm-ref/clases/semana_3_anexo_fisica_atencion.md */}

<CodeBlock
  title="energia_hopfield.py"
  language="python"
  code={`import torch

def calcular_energia(estado, pesos):
    # La energia es E = -0.5 * s.T * W * s
    energia = -0.5 * torch.dot(estado, pesos @ estado)
    return energia.item()

# Red con 3 neuronas
W = torch.tensor([[0.0, 1.0, -1.0],
                  [1.0, 0.0, 1.0],
                  [-1.0, 1.0, 0.0]])

recuerdo = torch.tensor([1.0, 1.0, -1.0])
ruido = torch.tensor([1.0, -1.0, -1.0])

print(f"Energia del recuerdo: {calcular_energia(recuerdo, W)}")
print(f"Energia con ruido:    {calcular_energia(ruido, W)}")`}
/>
