---
title: "06. Sistemas Din√°micos y F√≠sica Estad√≠stica"
order: 6
description: "De la Memoria Asociativa al Paisaje de Energ√≠a y Softmax."
---

# üåÄ Sistemas Din√°micos y Energ√≠a

¬øTe has preguntado alguna vez por qu√© una red neuronal puede "recordar" algo? Para entenderlo, tenemos que dejar de pensar como programadores y empezar a pensar como f√≠sicos. Vamos a mirar c√≥mo se comporta la materia para entender c√≥mo se comporta la inteligencia.

Ya te digo yo que esto es lo m√°s bonito de todo el curso: el **Modelo de Ising**. Es la historia de c√≥mo peque√±as unidades individuales (como √°tomos o neuronas) se ponen de acuerdo para crear un orden colectivo a partir del caos.

## 1. El Paisaje de Energ√≠a: Esculpiendo recuerdos
Imagina que el conocimiento es un mapa lleno de monta√±as y valles. Cuando un sistema f√≠sico busca la estabilidad, busca la **m√≠nima energ√≠a**. Se deja caer al fondo del valle.

En IA, aprender no es "guardar datos en un disco duro". Aprender es **moldear ese paisaje**. Est√°s usando el descenso del gradiente para cavar pozos en el mapa de energ√≠a de modo que, cuando le des a la red una imagen ruidosa, el sistema "caiga" de forma natural hacia el recuerdo correcto. ¬°Es una locura!

$$ E = - \sum_{i,j} J_{ij} s_i s_j $$

## 2. La Conexi√≥n con Hopfield
John Hopfield se dio cuenta de algo que cambi√≥ la historia: si tratamos a las neuronas como espines de un im√°n, podemos usar la f√≠sica para crear memoria.

<div className="concept-box">
### El Hamiltoniano: La partitura del sistema
En una Red de Hopfield, la energ√≠a total del sistema es nuestra gu√≠a:
$$ H = -\frac{1}{2} \sum_{i,j} w_{ij} s_i s_j $$
Recuperar un recuerdo es simplemente dejar que las neuronas "se relajen" y encuentren el estado de m√≠nima energ√≠a. Es como ver una canica rodar hasta el fondo de un cuenco.
</div>

## 3. Atractores, Softmax y Temperatura
En f√≠sica, un **atractor** es ese punto del valle que "atrae" todo lo que cae cerca. 
*   ¬øY la famosa funci√≥n **Softmax**? No es m√°s que una distribuci√≥n de Boltzmann disfrazada:
$$P(x) = \frac{1}{Z} e^{-E(x)/T}$$

Aqu√≠ entra en juego la **Temperatura ($T$)**. Si la temperatura es alta, hay mucho ruido y el sistema salta de un valle a otro. Si es baja, el sistema se queda "congelado" en el m√≠nimo. Por eso en los modelos de lenguaje (LLMs) ajustamos la "temperatura" para que sean m√°s creativos o m√°s deterministas. ¬°Es f√≠sica pura!

<div className="deep-explanation">
**Nota honesta:** Softmax no nacio de la fisica. Viene de regresion logistica y modelos probabilisticos. La lectura de Boltzmann es una interpretacion preciosa porque conecta energia y entropia, pero es a posteriori. Eso no la hace menos util; la hace mas profunda.
</div>


## 4. Referencias Acad√©micas
*   **Marc M√©zard & Andrea Montanari**, *Information, Physics, and Computation*. La biblia de esta conexi√≥n.
*   **John Hopfield**, *Neural networks and physical systems with emergent collective computational abilities* (1982).
*   **MIT OCW 8.044**, *Statistical Physics I*.
{/* Apuntes internos: llm-ref/clases/semana_3_anexo_fisica_atencion.md */}

<CodeBlock
  title="energia_hopfield.py"
  language="python"
  code={`import torch

def calcular_energia(estado, pesos):
    # La energia es E = -0.5 * s.T * W * s
    energia = -0.5 * torch.dot(estado, pesos @ estado)
    return energia.item()

# Red con 3 neuronas
W = torch.tensor([[0.0, 1.0, -1.0],
                  [1.0, 0.0, 1.0],
                  [-1.0, 1.0, 0.0]])

recuerdo = torch.tensor([1.0, 1.0, -1.0])
ruido = torch.tensor([1.0, -1.0, -1.0])

print(f"Energia del recuerdo: {calcular_energia(recuerdo, W)}")
print(f"Energia con ruido:    {calcular_energia(ruido, W)}")`}
/>
